Optimizer config: {'lr': 0.0001, 'weight_decay': 0.0, 'eps': 1e-08}
Set the loss scaled to False
Use step level LR & WD scheduler!
Set warmup steps = 2000
Auto resume checkpoint: 
Start training for 100 epochs, the global iterations is 0
Start training epoch 0, 2000 iters per inner epoch.
Epoch: [0]  [   0/2000]  eta: 10:49:54  lr: 0.000000  min_lr: 0.000000  vae_loss: 1.7769 (1.7769)  loss_scale: 1.0000 (1.0000)  total_loss: 1.7769 (1.7769)  logvar: 0.0000 (0.0000)  kl_loss: 17035.2656 (17035.2656)  nll_loss: 0.0734 (0.0734)  rec_loss: 0.0025 (0.0025)  perception_loss: 0.0481 (0.0481)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 9.2495 (9.2495)  time: 19.4971  data: 0.0001  max mem: 27513
Epoch: [0]  [  40/2000]  eta: 1:45:08  lr: 0.000002  min_lr: 0.000002  vae_loss: 1.8659 (1.8670)  loss_scale: 1.0000 (1.0000)  total_loss: 1.8659 (1.8670)  logvar: 0.0000 (0.0000)  kl_loss: 15922.3594 (16255.0099)  nll_loss: 0.2959 (0.2415)  rec_loss: 0.0218 (0.0176)  perception_loss: 0.0752 (0.0655)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 6.3221 (6.5104)  time: 2.7754  data: 0.0000  max mem: 31378
Epoch: [0]  [  80/2000]  eta: 1:35:51  lr: 0.000004  min_lr: 0.000004  vae_loss: 1.0125 (1.6176)  loss_scale: 1.0000 (1.0000)  total_loss: 1.0125 (1.6176)  logvar: 0.0000 (0.0000)  kl_loss: 9336.4111 (13948.6668)  nll_loss: 0.1301 (0.2227)  rec_loss: 0.0071 (0.0156)  perception_loss: 0.0596 (0.0665)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 8.7452 (7.6021)  time: 2.7735  data: 0.0000  max mem: 31378
Epoch: [0]  [ 120/2000]  eta: 1:31:14  lr: 0.000006  min_lr: 0.000006  vae_loss: 0.6305 (1.3369)  loss_scale: 1.0000 (1.0000)  total_loss: 0.6305 (1.3369)  logvar: 0.0000 (0.0000)  kl_loss: 3847.2825 (10923.4427)  nll_loss: 0.2181 (0.2445)  rec_loss: 0.0101 (0.0156)  perception_loss: 0.1235 (0.0883)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 6.9359 (8.8645)  time: 2.7405  data: 0.0000  max mem: 31380
Epoch: [0]  [ 160/2000]  eta: 1:28:04  lr: 0.000008  min_lr: 0.000008  vae_loss: 0.5293 (1.1486)  loss_scale: 1.0000 (1.0000)  total_loss: 0.5293 (1.1486)  logvar: 0.0000 (0.0000)  kl_loss: 2465.3772 (8889.1089)  nll_loss: 0.2654 (0.2597)  rec_loss: 0.0137 (0.0159)  perception_loss: 0.1387 (0.1006)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 4.7224 (8.0495)  time: 2.7741  data: 0.0000  max mem: 31380
Epoch: [0]  [ 200/2000]  eta: 1:25:25  lr: 0.000010  min_lr: 0.000010  vae_loss: 0.4572 (1.0258)  loss_scale: 1.0000 (1.0000)  total_loss: 0.4572 (1.0258)  logvar: 0.0000 (0.0000)  kl_loss: 2064.1133 (7535.0473)  nll_loss: 0.2506 (0.2723)  rec_loss: 0.0131 (0.0165)  perception_loss: 0.1309 (0.1073)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 5.8863 (7.6986)  time: 2.7332  data: 0.0000  max mem: 31380
Epoch: [0]  [ 240/2000]  eta: 1:23:13  lr: 0.000012  min_lr: 0.000012  vae_loss: 0.4057 (0.9293)  loss_scale: 1.0000 (1.0000)  total_loss: 0.4057 (0.9293)  logvar: 0.0000 (0.0000)  kl_loss: 1626.6157 (6574.5256)  nll_loss: 0.2426 (0.2719)  rec_loss: 0.0115 (0.0162)  perception_loss: 0.1226 (0.1096)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 6.7281 (7.6943)  time: 2.8240  data: 0.0000  max mem: 31381
Epoch: [0]  [ 280/2000]  eta: 1:20:58  lr: 0.000014  min_lr: 0.000014  vae_loss: 0.3984 (0.8601)  loss_scale: 1.0000 (1.0000)  total_loss: 0.3984 (0.8601)  logvar: 0.0000 (0.0000)  kl_loss: 1508.4507 (5864.4564)  nll_loss: 0.2264 (0.2736)  rec_loss: 0.0118 (0.0162)  perception_loss: 0.1177 (0.1120)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 7.0615 (7.5924)  time: 2.7496  data: 0.0000  max mem: 31382
Epoch: [0]  [ 320/2000]  eta: 1:18:31  lr: 0.000016  min_lr: 0.000016  vae_loss: 0.3738 (0.8035)  loss_scale: 1.0000 (1.0000)  total_loss: 0.3738 (0.8035)  logvar: 0.0000 (0.0000)  kl_loss: 1579.4075 (5333.4363)  nll_loss: 0.2192 (0.2701)  rec_loss: 0.0115 (0.0158)  perception_loss: 0.1108 (0.1121)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 5.7146 (7.4975)  time: 2.a: 0.0000  max mem: 31382

.8234  data: 0.0000  max mem: 31382

a: 0.0000  max mem: 31382

8398  data: 0.0000  max mem: 31382

_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 5.8863 (7.6986)  time: 2.7332  data: 0.0000  max mem: 31380
Epoch: [0]  [ 240/2000]  eta: 1:23:13  lr: 0.000012  min_lr: 0.000012  vae_loss: 0.4057 (0.9293)  loss_scale: 1.0000 (1.0000)  total_loss: 0.4057 (0.9293)  logvar: 0.0000 (0.0000)  kl_loss: 1626.6157 (6574.5256)  nll_loss: 0.2426 (0.2719)  rec_loss: 0.0115 (0.0162)  perception_loss: 0.1226 (0.1096)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 6.7281 (7.6943)  time: 2.8240  data: 0.0000  max mem: 31381
Epoch: [0]  [ 280/2000]  eta: 1:20:58  lr: 0.000014  min_lr: 0.000014  vae_loss: 0.3984 (0.8601)  loss_scale: 1.0000 (1.0000)  total_loss: 0.3984 (0.8601)  logvar: 0.0000 (0.0000)  kl_loss: 1508.4507 (5864.4564)  nll_loss: 0.2264 (0.2736)  rec_loss: 0.0118 (0.0162)  perception_loss: 0.1177 (0.1120)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 7.0615 (7.5924)  time: 2.7496  data: 0.0000  max mem: 31382
Epoch: [0]  [ 320/2000]  eta: 1:18:31  lr: 0.000016  min_lr: 0.000016  vae_loss: 0.3738 (0.8035)  loss_scale: 1.0000 (1.0000)  total_loss: 0.3738 (0.8035)  logvar: 0.0000 (0.0000)  kl_loss: 1579.4075 (5333.4363)  nll_loss: 0.2192 (0.2701)  rec_loss: 0.0115 (0.0158)  perception_loss: 0.1108 (0.1121)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 5.7146 (7.4975)  time: 2.5642  data: 0.0000  max mem: 31382
Epoch: [0]  [ 360/2000]  eta: 1:16:14  lr: 0.000018  min_lr: 0.000018  vae_loss: 0.4897 (0.7702)  loss_scale: 1.0000 (1.0000)  total_loss: 0.4897 (0.7702)  logvar: 0.0000 (0.0000)  kl_loss: 1663.7195 (4939.0702)  nll_loss: 0.3379 (0.2763)  rec_loss: 0.0249 (0.0165)  perception_loss: 0.0952 (0.1118)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 10.9414 (7.6380)  time: 2.8234  data: 0.0000  max mem: 31382
Epoch: [0]  [ 400/2000]  eta: 1:14:31  lr: 0.000020  min_lr: 0.000020  vae_loss: 0.5546 (0.7590)  loss_scale: 1.0000 (1.0000)  total_loss: 0.5546 (0.7590)  logvar: 0.0000 (0.0000)  kl_loss: 1439.4919 (4600.1210)  nll_loss: 0.4015 (0.2990)  rec_loss: 0.0315 (0.0188)  perception_loss: 0.0850 (0.1106)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 6.9296 (7.6300)  time: 2.8511  data: 0.0000  max mem: 31382
Epoch: [0]  [ 440/2000]  eta: 1:12:50  lr: 0.000022  min_lr: 0.000022  vae_loss: 0.5538 (0.7499)  loss_scale: 1.0000 (1.0000)  total_loss: 0.5538 (0.7499)  logvar: 0.0000 (0.0000)  kl_loss: 1375.7075 (4309.8444)  nll_loss: 0.4085 (0.3189)  rec_loss: 0.0321 (0.0210)  perception_loss: 0.0874 (0.1092)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 5.7446 (7.5277)  time: 2.8398  data: 0.0000  max mem: 31382
Epoch: [0]  [ 480/2000]  eta: 1:10:59  lr: 0.000024  min_lr: 0.000024  vae_loss: 0.7034 (0.7502)  loss_scale: 1.0000 (1.0000)  total_loss: 0.7034 (0.7502)  logvar: 0.0000 (0.0000)  kl_loss: 1327.0083 (4059.4494)  nll_loss: 0.5599 (0.3443)  rec_loss: 0.0471 (0.0235)  perception_loss: 0.1113 (0.1089)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 5.3667 (7.4280)  time: 2.8262  data: 0.0000  max mem: 31382
Epoch: [0]  [ 520/2000]  eta: 1:09:13  lr: 0.000026  min_lr: 0.000026  vae_loss: 0.6896 (0.7462)  loss_scale: 1.0000 (1.0000)  total_loss: 0.6896 (0.7462)  logvar: 0.0000 (0.0000)  kl_loss: 1240.7097 (3842.9021)  nll_loss: 0.5766 (0.3620)  rec_loss: 0.0471 (0.0254)  perception_loss: 0.0986 (0.1083)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 5.8605 (7.3899)  time: 2.8818  data: 0.0000  max mem: 31382