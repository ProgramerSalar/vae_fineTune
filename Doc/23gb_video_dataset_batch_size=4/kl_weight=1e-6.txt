/content/vae_fineTune# sh scripts/train_causal_video_vae.sh
/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Load the base VideoVAE checkpoint from path: PATH/vae_ckpt, using dtype bf16
The latent dimmension channes is 16
/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
loaded pretrained LPIPS loss from PATH/vgg_lpips.pth
The training video clip frame number is 6 
Load annotation file from annotation/video_data_files_path.jsonl
14893it [00:00, 521164.79it/s]
Totally Remained 14893 videos
Transform List is [Resize(size=256, interpolation=bicubic, max_size=None, antialias=True), CenterCrop(size=(256, 256)), Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))]
loss.perceptual_loss.net.slice1.0.weight
loss.perceptual_loss.net.slice1.0.bias
loss.perceptual_loss.net.slice1.2.weight
loss.perceptual_loss.net.slice1.2.bias
loss.perceptual_loss.net.slice2.5.weight
loss.perceptual_loss.net.slice2.5.bias
loss.perceptual_loss.net.slice2.7.weight
loss.perceptual_loss.net.slice2.7.bias
loss.perceptual_loss.net.slice3.10.weight
loss.perceptual_loss.net.slice3.10.bias
loss.perceptual_loss.net.slice3.12.weight
loss.perceptual_loss.net.slice3.12.bias
loss.perceptual_loss.net.slice3.14.weight
loss.perceptual_loss.net.slice3.14.bias
loss.perceptual_loss.net.slice4.17.weight
loss.perceptual_loss.net.slice4.17.bias
loss.perceptual_loss.net.slice4.19.weight
loss.perceptual_loss.net.slice4.19.bias
loss.perceptual_loss.net.slice4.21.weight
loss.perceptual_loss.net.slice4.21.bias
loss.perceptual_loss.net.slice5.24.weight
loss.perceptual_loss.net.slice5.24.bias
loss.perceptual_loss.net.slice5.26.weight
loss.perceptual_loss.net.slice5.26.bias
loss.perceptual_loss.net.slice5.28.weight
loss.perceptual_loss.net.slice5.28.bias
loss.perceptual_loss.lin0.model.1.weight
loss.perceptual_loss.lin1.model.1.weight
loss.perceptual_loss.lin2.model.1.weight
loss.perceptual_loss.lin3.model.1.weight
loss.perceptual_loss.lin4.model.1.weight
total number of learnable params: 335.402068 M

      "encoder.mid_block.attentions.0.to_v.weight",
      "encoder.mid_block.attentions.0.to_out.0.weight",
      "encoder.mid_block.resnets.0.conv1.conv.weight",
      "encoder.mid_block.resnets.0.conv2.conv.weight",
      "encoder.mid_block.resnets.1.conv1.conv.weight",
      "encoder.mid_block.resnets.1.conv2.conv.weight",
      "encoder.conv_out.conv.weight",
      "decoder.conv_in.conv.weight",
      "decoder.up_blocks.0.resnets.0.conv1.conv.weight",
      "decoder.up_blocks.0.resnets.0.conv2.conv.weight",
      "decoder.up_blocks.0.resnets.1.conv1.conv.weight",
      "decoder.up_blocks.0.resnets.1.conv2.conv.weight",
      "decoder.up_blocks.0.resnets.2.conv1.conv.weight",
      "decoder.up_blocks.0.resnets.2.conv2.conv.weight",
      "decoder.up_blocks.0.upsamplers.0.conv.conv.weight",
      "decoder.up_blocks.0.temporal_upsamplers.0.conv.conv.weight",
      "decoder.up_blocks.1.resnets.0.conv1.conv.weight",
      "decoder.up_blocks.1.resnets.0.conv2.conv.weight",
      "decoder.up_blocks.1.resnets.1.conv1.conv.weight",
      "decoder.up_blocks.1.resnets.1.conv2.conv.weight",
      "decoder.up_blocks.1.resnets.2.conv1.conv.weight",
      "decoder.up_blocks.1.resnets.2.conv2.conv.weight",
      "decoder.up_blocks.1.upsamplers.0.conv.conv.weight",
      "decoder.up_blocks.1.temporal_upsamplers.0.conv.conv.weight",
      "decoder.up_blocks.2.resnets.0.conv1.conv.weight",
      "decoder.up_blocks.2.resnets.0.conv2.conv.weight",
      "decoder.up_blocks.2.resnets.0.conv_shortcut.conv.weight",
      "decoder.up_blocks.2.resnets.1.conv1.conv.weight",
      "decoder.up_blocks.2.resnets.1.conv2.conv.weight",
      "decoder.up_blocks.2.resnets.2.conv1.conv.weight",
      "decoder.up_blocks.2.resnets.2.conv2.conv.weight",
      "decoder.up_blocks.2.upsamplers.0.conv.conv.weight",
      "decoder.up_blocks.2.temporal_upsamplers.0.conv.conv.weight",
      "decoder.up_blocks.3.resnets.0.conv1.conv.weight",
      "decoder.up_blocks.3.resnets.0.conv2.conv.weight",
      "decoder.up_blocks.3.resnets.0.conv_shortcut.conv.weight",
      "decoder.up_blocks.
      "encoder.down_blocks.1.resnets.0.conv_shortcut.conv.bias",
      "encoder.down_blocks.1.resnets.1.norm1.weight",
      "encoder.down_blocks.1.resnets.1.norm1.bias",
      "encoder.down_blocks.1.resnets.1.conv1.conv.bias",
      "encoder.down_blocks.1.resnets.1.norm2.weight",
      "encoder.down_blocks.1.resnets.1.norm2.bias",
      "encoder.down_blocks.1.resnets.1.conv2.conv.bias",
      "encoder.down_blocks.1.downsamplers.0.conv.conv.bias",
      "encoder.down_blocks.1.temporal_downsamplers.0.conv.conv.bias",
      "encoder.down_blocks.2.resnets.0.norm1.weight",
      "encoder.down_blocks.2.resnets.0.norm1.bias",
      "encoder.down_blocks.2.resnets.0.conv1.conv.bias",
      "encoder.down_blocks.2.resnets.0.norm2.weight",
      "encoder.down_blocks.2.resnets.0.norm2.bias",
      "encoder.down_blocks.2.resnets.0.conv2.conv.bias",
      "encoder.down_blocks.2.resnets.0.conv_shortcut.conv.bias",
      "encoder.down_blocks.2.resnets.1.norm1.weight",
      "encoder.down_blocks.2.resnets.1.norm1.bias",
      "encoder.down_blocks.2.resnets.1.conv1.conv.bias",
      "encoder.down_blocks.2.resnets.1.norm2.weight",
      "encoder.down_blocks.2.resnets.1.norm2.bias",
      "encoder.down_blocks.2.resnets.1.conv2.conv.bias",
      "encoder.down_blocks.2.downsamplers.0.conv.conv.bias",
      "encoder.down_blocks.2.temporal_downsamplers.0.conv.conv.bias",
      "encoder.down_blocks.3.resnets.0.norm1.weight",
      "encoder.down_blocks.3.resnets.0.norm1.bias",
      "encoder.down_blocks.3.resnets.0.conv1.conv.bias",
      "encoder.down_blocks.3.resnets.0.norm2.weight",
      "encoder.down_blocks.3.resnets.0.norm2.bias",
      "encoder.down_blocks.3.resnets.0.conv2.conv.bias",
      "encoder.down_blocks.3.resnets.1.norm1.weight",
      "encoder.down_blocks.3.resnets.1.norm1.bias",
      "encoder.down_blocks.3.resnets.1.conv1.conv.bias",
      "encoder.down_blocks.3.resnets.1.norm2.weight",
      "encoder.down_blocks.3.resnets.1.norm2.bias",
      "encoder.down_blocks.3.resnets.1.conv2.conv.bias",
  
      "decoder.up_blocks.0.temporal_upsamplers.0.conv.conv.bias",
      "decoder.up_blocks.1.resnets.0.norm1.weight",
      "decoder.up_blocks.1.resnets.0.norm1.bias",
      "decoder.up_blocks.1.resnets.0.conv1.conv.bias",
      "decoder.up_blocks.1.resnets.0.norm2.weight",
      "decoder.up_blocks.1.resnets.0.norm2.bias",
      "decoder.up_blocks.1.resnets.0.conv2.conv.bias",
      "decoder.up_blocks.1.resnets.1.norm1.weight",
      "decoder.up_blocks.1.resnets.1.norm1.bias",
      "decoder.up_blocks.1.resnets.1.conv1.conv.bias",
      "decoder.up_blocks.1.resnets.1.norm2.weight",
      "decoder.up_blocks.1.resnets.1.norm2.bias",
      "decoder.up_blocks.1.resnets.1.conv2.conv.bias",
      "decoder.up_blocks.1.resnets.2.norm1.weight",
      "decoder.up_blocks.1.resnets.2.norm1.bias",
      "decoder.up_blocks.1.resnets.2.conv1.conv.bias",
      "decoder.up_blocks.1.resnets.2.norm2.weight",
      "decoder.up_blocks.1.resnets.2.norm2.bias",
      "decoder.up_blocks.1.resnets.2.conv2.conv.bias",
      "decoder.up_blocks.1.upsamplers.0.conv.conv.bias",
      "decoder.up_blocks.1.temporal_upsamplers.0.conv.conv.bias",
      "decoder.up_blocks.2.resnets.0.norm1.weight",
      "decoder.up_blocks.2.resnets.0.norm1.bias",
      "decoder.up_blocks.2.resnets.0.conv1.conv.bias",
      "decoder.up_blocks.2.resnets.0.norm2.weight",
      "decoder.up_blocks.2.resnets.0.norm2.bias",
      "decoder.up_blocks.2.resnets.0.conv2.conv.bias",
      "decoder.up_blocks.2.resnets.0.conv_shortcut.conv.bias",
      "decoder.up_blocks.2.resnets.1.norm1.weight",
      "decoder.up_blocks.2.resnets.1.norm1.bias",
      "decoder.up_blocks.2.resnets.1.conv1.conv.bias",
      "decoder.up_blocks.2.resnets.1.norm2.weight",
      "decoder.up_blocks.2.resnets.1.norm2.bias",
      "decoder.up_blocks.2.resnets.1.conv2.conv.bias",
      "decoder.up_blocks.2.resnets.2.norm1.weight",
      "decoder.up_blocks.2.resnets.2.norm1.bias",
      "decoder.up_blocks.2.res
      "decoder.up_blocks.3.resnets.1.conv2.conv.bias",
      "decoder.up_blocks.3.resnets.2.norm1.weight",
      "decoder.up_blocks.3.resnets.2.norm1.bias",
      "decoder.up_blocks.3.resnets.2.conv1.conv.bias",
      "decoder.up_blocks.3.resnets.2.norm2.weight",
      "decoder.up_blocks.3.resnets.2.norm2.bias",
      "decoder.up_blocks.3.resnets.2.conv2.conv.bias",
      "decoder.mid_block.attentions.0.group_norm.weight",
      "decoder.mid_block.attentions.0.group_norm.bias",
      "decoder.mid_block.attentions.0.to_q.bias",
      "decoder.mid_block.attentions.0.to_k.bias",
      "decoder.mid_block.attentions.0.to_v.bias",
      "decoder.mid_block.attentions.0.to_out.0.bias",
      "decoder.mid_block.resnets.0.norm1.weight",
      "decoder.mid_block.resnets.0.norm1.bias",
      "decoder.mid_block.resnets.0.conv1.conv.bias",
      "decoder.mid_block.resnets.0.norm2.weight",
      "decoder.mid_block.resnets.0.norm2.bias",
      "decoder.mid_block.resnets.0.conv2.conv.bias",
      "decoder.mid_block.resnets.1.norm1.weight",
      "decoder.mid_block.resnets.1.norm1.bias",
      "decoder.mid_block.resnets.1.conv1.conv.bias",
      "decoder.mid_block.resnets.1.norm2.weight",
      "decoder.mid_block.resnets.1.norm2.bias",
      "decoder.mid_block.resnets.1.conv2.conv.bias",
      "decoder.conv_norm_out.weight",
      "decoder.conv_norm_out.bias",
      "decoder.conv_out.conv.bias",
      "quant_conv.conv.bias",
      "post_quant_conv.conv.bias"
    ],
    "lr": 0.0001,
    "lr_scale": 1.0
  }
}
Optimizer config: {'lr': 0.0001, 'weight_decay': 0.0, 'eps': 1e-08}
Set the loss scaled to False
/content/vae_fineTune/trainer_misc/utils.py:419: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler(enabled=enabled)
Use step level LR & WD scheduler!
Set warmup steps = 2000
Auto resume checkpoint: 
Start training for 100 epochs, the global iterations is 0
Start training epoch 0, 2000 iters per inner epoch.
Epoch: [0]  [   0/2000]  eta: 9:37:07  lr: 0.000000  min_lr: 0.000000  vae_loss: 0.0905 (0.0905)  loss_scale: 1.0000 (1.0000)  total_loss: 0.0905 (0.0905)  logvar: 0.0000 (0.0000)  kl_loss: 17035.2656 (17035.2656)  nll_loss: 0.0734 (0.0734)  rec_loss: 0.0025 (0.0025)  perception_loss: 0.0481 (0.0481)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 4.8735 (4.8735)  time: 17.3138  data: 0.0001  max mem: 27513
Epoch: [0]  [  40/2000]  eta: 1:42:12  lr: 0.000002  min_lr: 0.000002  vae_loss: 0.3099 (0.2569)  loss_scale: 1.0000 (1.0000)  total_loss: 0.3099 (0.2569)  logvar: 0.0000 (0.0000)  kl_loss: 16613.0117 (16704.5632)  nll_loss: 0.2934 (0.2402)  rec_loss: 0.0217 (0.0175)  perception_loss: 0.0747 (0.0654)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 2.3896 (2.7028)  time: 2.7295  data: 0.0000  max mem: 31378
Epoch: [0]  [  80/2000]  eta: 1:33:54  lr: 0.000004  min_lr: 0.000004  vae_loss: 0.1161 (0.2325)  loss_scale: 1.0000 (1.0000)  total_loss: 0.1161 (0.2325)  logvar: 0.0000 (0.0000)  kl_loss: 15955.2715 (16466.0548)  nll_loss: 0.0997 (0.2160)  rec_loss: 0.0062 (0.0153)  perception_loss: 0.0486 (0.0630)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 1.8769 (2.5154)  time: 2.7189  data: 0.0000  max mem: 31378
Epoch: [0]  [ 120/2000]  eta: 1:29:44  lr: 0.000006  min_lr: 0.000006  vae_loss: 0.1526 (0.2217)  loss_scale: 1.0000 (1.0000)  total_loss: 0.1526 (0.2217)  logvar: 0.0000 (0.0000)  kl_loss: 13880.3594 (15804.6983)  nll_loss: 0.1393 (0.2059)  rec_loss: 0.0082 (0.0145)  perception_loss: 0.0508 (0.0610)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 3.4995 (2.7407)  time: 2.7124  data: 0.0000  max mem: 31380
Epoch: [0]  [ 160/2000]  eta: 1:26:41  lr: 0.000008  min_lr: 0.000008  vae_loss: 0.1798 (0.2205)  loss_scale: 1.0000 (1.0000)  total_loss: 0.1798 (0.2205)  logvar: 0.0000 (0.0000)  kl_loss: 10762.3350 (14778.6300)  nll_loss: 0.1686 (0.2057)  rec_loss: 0.0116 (0.0144)  perception_loss: 0.0547 (0.0612)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 2.3055 (2.7026)  time: 2.7394  data: 0.0000  max mem: 31380
Epoch: [0]  [ 200/2000]  eta: 1:24:14  lr: 0.000010  min_lr: 0.000010  vae_loss: 0.1774 (0.2238)  loss_scale: 1.0000 (1.0000)  total_loss: 0.1774 (0.2238)  logvar: 0.0000 (0.0000)  kl_loss: 8909.8320 (13675.5162)  nll_loss: 0.1682 (0.2102)  rec_loss: 0.0111 (0.0148)  perception_loss: 0.0571 (0.0620)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 4.7267 (2.9226)  time: 2.7316  data: 0.0000  max mem: 31380
Epoch: [0]  [ 240/2000]  eta: 1:21:59  lr: 0.000012  min_lr: 0.000012  vae_loss: 0.1613 (0.2180)  loss_scale: 1.0000 (1.0000)  total_loss: 0.1613 (0.2180)  logvar: 0.0000 (0.0000)  kl_loss: 7977.1133 (12772.9421)  nll_loss: 0.1533 (0.2052)  rec_loss: 0.0095 (0.0144)  perception_loss: 0.0581 (0.0612)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 3.1728 (3.1305)  time: 2.7417  data: 0.0000  max mem: 31381
Epoch: [0]  [ 280/2000]  eta: 1:19:45  lr: 0.000014  min_lr: 0.000014  vae_loss: 0.1526 (0.2161)  loss_scale: 1.0000 (1.0000)  total_loss: 0.1526 (0.2161)  logvar: 0.0000 (0.0000)  kl_loss: 7553.1685 (12062.9677)  nll_loss: 0.1443 (0.2040)  rec_loss: 0.0090 (0.0143)  perception_loss: 0.0542 (0.0614)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 3.9090 (3.3731)  time: 2.7024  data: 0.0000  max mem: 31382
Epoch: [0]  [ 320/2000]  eta: 1:17:25  lr: 0.000016  min_lr: 0.000016  vae_loss: 0.1504 (0.2099)  loss_scale: 1.0000 (1.0000)  total_loss: 0.1504 (0.2099)  logvar: 0.0000 (0.0000)  kl_loss: 7438.4858 (11501.8936)  nll_loss: 0.1433 (0.1984)  rec_loss: 0.0087 (0.0138)  perception_loss: 0.0527 (0.0605)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 2.8455 (3.3964)  time: 2.5474  data: 0.0000  max mem: 31382
Epoch: [0]  [ 360/2000]  eta: 1:15:14  lr: 0.000018  min_lr: 0.000018  vae_loss: 0.2244 (0.2128)  loss_scale: 1.0000 (1.0000)  total_loss: 0.2244 (0.2128)  logvar: 0.0000 (0.0000)  kl_loss: 7819.4038 (11060.8207)  nll_loss: 0.2177 (0.2018)  rec_loss: 0.0176 (0.0141)  perception_loss: 0.0425 (0.0603)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 5.4197 (3.5180)  time: 2.8034  data: 0.0000  max mem: 31382
Epoch: [0]  [ 400/2000]  eta: 1:13:29  lr: 0.000020  min_lr: 0.000020  vae_loss: 0.3265 (0.2346)  loss_scale: 1.0000 (1.0000)  total_loss: 0.3265 (0.2346)  logvar: 0.0000 (0.0000)  kl_loss: 6858.5205 (10712.2085)  nll_loss: 0.3187 (0.2238)  rec_loss: 0.0271 (0.0163)  perception_loss: 0.0537 (0.0607)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 4.0847 (3.6665)  time: 2.7967  data: 0.0000  max mem: 31382
Epoch: [0]  [ 440/2000]  eta: 1:11:45  lr: 0.000022  min_lr: 0.000022  vae_loss: 0.3278 (0.2539)  loss_scale: 1.0000 (1.0000)  total_loss: 0.3278 (0.2539)  logvar: 0.0000 (0.0000)  kl_loss: 6466.5049 (10332.9574)  nll_loss: 0.3212 (0.2436)  rec_loss: 0.0266 (0.0182)  perception_loss: 0.0569 (0.0611)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 3.8530 (3.7004)  time: 2.7626  data: 0.0000  max mem: 31382
Epoch: [0]  [ 480/2000]  eta: 1:09:54  lr: 0.000024  min_lr: 0.000024  vae_loss: 0.4935 (0.2790)  loss_scale: 1.0000 (1.0000)  total_loss: 0.4935 (0.2790)  logvar: 0.0000 (0.0000)  kl_loss: 6287.3633 (9993.0253)  nll_loss: 0.4872 (0.2690)  rec_loss: 0.0421 (0.0207)  perception_loss: 0.0811 (0.0623)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 4.2780 (3.7503)  time: 2.7449  data: 0.0000  max mem: 31382
Epoch: [0]  [ 520/2000]  eta: 1:08:08  lr: 0.000026  min_lr: 0.000026  vae_loss: 0.5036 (0.2963)  loss_scale: 1.0000 (1.0000)  total_loss: 0.5036 (0.2963)  logvar: 0.0000 (0.0000)  kl_loss: 6020.6191 (9690.5498)  nll_loss: 0.4975 (0.2866)  rec_loss: 0.0422 (0.0224)  perception_loss: 0.0718 (0.0631)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 3.6203 (3.7532)  time: 2.8209  data: 0.0000  max mem: 31382
Epoch: [0]  [ 560/2000]  eta: 1:06:21  lr: 0.000028  min_lr: 0.000028  vae_loss: 0.5123 (0.3111)  loss_scale: 1.0000 (1.0000)  total_loss: 0.5123 (0.3111)  logvar: 0.0000 (0.0000)  kl_loss: 5877.1479 (9418.6022)  nll_loss: 0.5065 (0.3017)  rec_loss: 0.0426 (0.0238)  perception_loss: 0.0757 (0.0638)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 3.9732 (3.8064)  time: 2.8154  data: 0.0000  max mem: 31382
Epoch: [0]  [ 600/2000]  eta: 1:04:15  lr: 0.000030  min_lr: 0.000030  vae_loss: 0.0719 (0.3119)  loss_scale: 1.0000 (1.0000)  total_loss: 0.0719 (0.3119)  logvar: 0.0000 (0.0000)  kl_loss: 6227.5469 (9195.6378)  nll_loss: 0.0654 (0.3027)  rec_loss: 0.0021 (0.0239)  perception_loss: 0.0410 (0.0633)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 8.2673 (3.9936)  time: 2.5451  data: 0.0000  max mem: 31382
Epoch: [0]  [ 640/2000]  eta: 1:02:48  lr: 0.000032  min_lr: 0.000032  vae_loss: 0.1165 (0.3005)  loss_scale: 1.0000 (1.0000)  total_loss: 0.1165 (0.3005)  logvar: 0.0000 (0.0000)  kl_loss: 6042.2983 (9007.5589)  nll_loss: 0.1101 (0.2915)  rec_loss: 0.0074 (0.0230)  perception_loss: 0.0330 (0.0619)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 3.7255 (4.0429)  time: 3.0042  data: 0.0000  max mem: 31382
Epoch: [0]  [ 680/2000]  eta: 1:00:57  lr: 0.000034  min_lr: 0.000034  vae_loss: 0.1933 (0.2949)  loss_scale: 1.0000 (1.0000)  total_loss: 0.1933 (0.2949)  logvar: 0.0000 (0.0000)  kl_loss: 5815.0786 (8826.8105)  nll_loss: 0.1876 (0.2861)  rec_loss: 0.0115 (0.0224)  perception_loss: 0.0645 (0.0621)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 4.1620 (4.1009)  time: 2.7522  data: 0.0000  max mem: 31382
Epoch: [0]  [ 720/2000]  eta: 0:59:09  lr: 0.000036  min_lr: 0.000036  vae_loss: 0.1840 (0.2900)  loss_scale: 1.0000 (1.0000)  total_loss: 0.1840 (0.2900)  logvar: 0.0000 (0.0000)  kl_loss: 5615.0737 (8653.7524)  nll_loss: 0.1782 (0.2814)  rec_loss: 0.0124 (0.0219)  perception_loss: 0.0540 (0.0622)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 3.9022 (4.0936)  time: 2.8057  data: 0.0000  max mem: 31382
Epoch: [0]  [ 760/2000]  eta: 0:57:19  lr: 0.000038  min_lr: 0.000038  vae_loss: 0.1846 (0.2853)  loss_scale: 1.0000 (1.0000)  total_loss: 0.1846 (0.2853)  logvar: 0.0000 (0.0000)  kl_loss: 5478.8247 (8488.6489)  nll_loss: 0.1790 (0.2768)  rec_loss: 0.0119 (0.0215)  perception_loss: 0.0613 (0.0622)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 3.7776 (4.0879)  time: 2.7963  data: 0.0000  max mem: 31382
Epoch: [0]  [ 800/2000]  eta: 0:55:32  lr: 0.000040  min_lr: 0.000040  vae_loss: 0.1922 (0.2815)  loss_scale: 1.0000 (1.0000)  total_loss: 0.1922 (0.2815)  logvar: 0.0000 (0.0000)  kl_loss: 5370.2197 (8335.7585)  nll_loss: 0.1868 (0.2732)  rec_loss: 0.0111 (0.0211)  perception_loss: 0.0649 (0.0624)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 1.8254 (4.0204)  time: 2.8660  data: 0.0000  max mem: 31382
Epoch: [0]  [ 840/2000]  eta: 0:53:42  lr: 0.000042  min_lr: 0.000042  vae_loss: 0.1735 (0.2781)  loss_scale: 1.0000 (1.0000)  total_loss: 0.1735 (0.2781)  logvar: 0.0000 (0.0000)  kl_loss: 5339.1279 (8193.1721)  nll_loss: 0.1683 (0.2699)  rec_loss: 0.0107 (0.0207)  perception_loss: 0.0566 (0.0626)  d_weight: 0.0000 (0.0000)  disc_factor: 0.0000 (0.0000)  g_loss: 0.0000 (0.0000)  weight_decay: 0.0010 (0.0010)  grad_norm: 2.9336 (3.9871)  time: 2.7795  data: 0.0000  max mem: 31382